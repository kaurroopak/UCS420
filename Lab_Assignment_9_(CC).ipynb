{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyO/LpT/9g9ytKlvrwf5CNzB",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kaurroopak/UCS420/blob/main/Lab_Assignment_9_(CC).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Lab Assignment 9\n",
        "###Submitted by :- Roopakjeet Kaur (102497013)"
      ],
      "metadata": {
        "id": "y0zQ0MWTu0GJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q1. Write a unique paragraph (5-6 sentences) about your favorite topic (e.g., sports, technology, food, books, etc.).\n",
        "1. Convert text to lowercase and remove punctuation.\n",
        "2. Tokenize the text into words and sentences.\n",
        "3. Remove stopwords (using NLTK's stopwords list).\n",
        "4. Display word frequency distribution (excluding stopwords)."
      ],
      "metadata": {
        "id": "E9MSMHKdu8iE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.tokenize import word_tokenize, sent_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from collections import Counter\n",
        "import string\n",
        "\n",
        "nltk.download('punkt_tab')\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "\n",
        "# Original paragraph\n",
        "paragraph = \"\"\"\n",
        "Food has always been a passion of mine. I love exploring different cuisines from around the world,\n",
        "especially spicy Indian curries and savory Italian pastas. The experience of tasting new dishes excites me,\n",
        "whether it's street food or gourmet meals. Cooking at home also gives me a sense of creativity and satisfaction.\n",
        "I enjoy experimenting with flavors, textures, and ingredients. To me, food is not just fuel,\n",
        "but an art form and a way to connect with people.\n",
        "\"\"\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F4555ib-vpiu",
        "outputId": "00ab4ffb-9467-4254-bcbb-7f6ea56f988a"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. Lowercase and remove punctuation\n",
        "paragraph_lower = paragraph.lower()\n",
        "paragraph_clean = paragraph_lower.translate(str.maketrans('', '', string.punctuation))\n",
        "\n",
        "# 2. Tokenize into sentences and words\n",
        "sentences = sent_tokenize(paragraph_clean)\n",
        "words = word_tokenize(paragraph_clean)\n",
        "\n",
        "# 3. Remove stopwords\n",
        "stop_words = set(stopwords.words('english'))\n",
        "filtered_words = [word for word in words if word not in stop_words]\n",
        "\n",
        "# 4. Word frequency distribution\n",
        "word_freq = Counter(filtered_words)\n",
        "\n",
        "# Output\n",
        "print(\"Tokenized Sentences:\\n\", sentences)\n",
        "print(\"\\nFiltered Words (no stopwords):\\n\", filtered_words)\n",
        "print(\"\\nWord Frequency Distribution:\")\n",
        "for word, freq in word_freq.items():\n",
        "    print(f\"{word}: {freq}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L2bNXXF_wL7x",
        "outputId": "a5b79770-00b3-4b4e-8815-ac027f193535"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tokenized Sentences:\n",
            " ['\\nfood has always been a passion of mine i love exploring different cuisines from around the world \\nespecially spicy indian curries and savory italian pastas the experience of tasting new dishes excites me \\nwhether its street food or gourmet meals cooking at home also gives me a sense of creativity and satisfaction \\ni enjoy experimenting with flavors textures and ingredients to me food is not just fuel \\nbut an art form and a way to connect with people']\n",
            "\n",
            "Filtered Words (no stopwords):\n",
            " ['food', 'always', 'passion', 'mine', 'love', 'exploring', 'different', 'cuisines', 'around', 'world', 'especially', 'spicy', 'indian', 'curries', 'savory', 'italian', 'pastas', 'experience', 'tasting', 'new', 'dishes', 'excites', 'whether', 'street', 'food', 'gourmet', 'meals', 'cooking', 'home', 'also', 'gives', 'sense', 'creativity', 'satisfaction', 'enjoy', 'experimenting', 'flavors', 'textures', 'ingredients', 'food', 'fuel', 'art', 'form', 'way', 'connect', 'people']\n",
            "\n",
            "Word Frequency Distribution:\n",
            "food: 3\n",
            "always: 1\n",
            "passion: 1\n",
            "mine: 1\n",
            "love: 1\n",
            "exploring: 1\n",
            "different: 1\n",
            "cuisines: 1\n",
            "around: 1\n",
            "world: 1\n",
            "especially: 1\n",
            "spicy: 1\n",
            "indian: 1\n",
            "curries: 1\n",
            "savory: 1\n",
            "italian: 1\n",
            "pastas: 1\n",
            "experience: 1\n",
            "tasting: 1\n",
            "new: 1\n",
            "dishes: 1\n",
            "excites: 1\n",
            "whether: 1\n",
            "street: 1\n",
            "gourmet: 1\n",
            "meals: 1\n",
            "cooking: 1\n",
            "home: 1\n",
            "also: 1\n",
            "gives: 1\n",
            "sense: 1\n",
            "creativity: 1\n",
            "satisfaction: 1\n",
            "enjoy: 1\n",
            "experimenting: 1\n",
            "flavors: 1\n",
            "textures: 1\n",
            "ingredients: 1\n",
            "fuel: 1\n",
            "art: 1\n",
            "form: 1\n",
            "way: 1\n",
            "connect: 1\n",
            "people: 1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q2: Stemming and Lemmatization\n",
        "1. Take the tokenized words from Question 1 (atier stopword removal).\n",
        "2. Apply stemming using NLTK's PorterStemmer and LancasterStemmer.\n",
        "3. Apply lemmatization using NLTK's WordNetLemmatizer.\n",
        "4. Compare and display results of both techniques."
      ],
      "metadata": {
        "id": "EKwuGEsownCU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import PorterStemmer, LancasterStemmer, WordNetLemmatizer\n",
        "from nltk.corpus import wordnet\n",
        "\n",
        "nltk.download('wordnet')\n",
        "nltk.download('omw-1.4')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sl8xwEfxw0BY",
        "outputId": "bcb160e6-1d9f-4892-b40d-551eba386311"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "porter = PorterStemmer()\n",
        "lancaster = LancasterStemmer()\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "# Apply stemming and lemmatization\n",
        "print(f\"{'Original':<15}{'Porter':<15}{'Lancaster':<15}{'Lemmatizer'}\")\n",
        "print(\"-\" * 60)\n",
        "\n",
        "for word in filtered_words: # filtered_words found in Q1\n",
        "    porter_stem = porter.stem(word)\n",
        "    lancaster_stem = lancaster.stem(word)\n",
        "    lemma = lemmatizer.lemmatize(word)\n",
        "    print(f\"{word:<15}{porter_stem:<15}{lancaster_stem:<15}{lemma}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UnWcg7UPxBaa",
        "outputId": "549d67f1-da2b-4504-b61a-11817d7febbb"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original       Porter         Lancaster      Lemmatizer\n",
            "------------------------------------------------------------\n",
            "food           food           food           food\n",
            "always         alway          alway          always\n",
            "passion        passion        pass           passion\n",
            "mine           mine           min            mine\n",
            "love           love           lov            love\n",
            "exploring      explor         expl           exploring\n",
            "different      differ         diff           different\n",
            "cuisines       cuisin         cuisin         cuisine\n",
            "around         around         around         around\n",
            "world          world          world          world\n",
            "especially     especi         espec          especially\n",
            "spicy          spici          spicy          spicy\n",
            "indian         indian         ind            indian\n",
            "curries        curri          curry          curry\n",
            "savory         savori         sav            savory\n",
            "italian        italian        it             italian\n",
            "pastas         pasta          pasta          pasta\n",
            "experience     experi         expery         experience\n",
            "tasting        tast           tast           tasting\n",
            "new            new            new            new\n",
            "dishes         dish           dish           dish\n",
            "excites        excit          excit          excites\n",
            "whether        whether        wheth          whether\n",
            "street         street         street         street\n",
            "food           food           food           food\n",
            "gourmet        gourmet        gourmet        gourmet\n",
            "meals          meal           meal           meal\n",
            "cooking        cook           cook           cooking\n",
            "home           home           hom            home\n",
            "also           also           also           also\n",
            "gives          give           giv            give\n",
            "sense          sens           sens           sense\n",
            "creativity     creativ        cre            creativity\n",
            "satisfaction   satisfact      satisfact      satisfaction\n",
            "enjoy          enjoy          enjoy          enjoy\n",
            "experimenting  experi         expery         experimenting\n",
            "flavors        flavor         flav           flavor\n",
            "textures       textur         text           texture\n",
            "ingredients    ingredi        ingredy        ingredient\n",
            "food           food           food           food\n",
            "fuel           fuel           fuel           fuel\n",
            "art            art            art            art\n",
            "form           form           form           form\n",
            "way            way            way            way\n",
            "connect        connect        connect        connect\n",
            "people         peopl          peopl          people\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q3. Regular Expressions and Text Spliting\n",
        "1. Take their original text from Question 1.\n",
        "2. Use regular expressions to:\n",
        "\n",
        "  a. Extract all words with more than 5 letters.\n",
        "\n",
        "  b. Extract all numbers (if any exist in their text).\n",
        "\n",
        "  c. Extract all capitalized words.\n",
        "3. Use text spliÆ«ng techniques to:\n",
        "\n",
        "  a. Split the text into words containing only alphabets (removing digits and special characters).\n",
        "\n",
        "  b. Extract words starting with a vowel."
      ],
      "metadata": {
        "id": "scItrH9qxRWl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "\n",
        "# 1.\n",
        "text = paragraph\n",
        "\n",
        "# 2a.\n",
        "words_over_5 = re.findall(r'\\b[a-zA-Z]{6,}\\b', text)\n",
        "\n",
        "# 2b.\n",
        "numbers = re.findall(r'\\b\\d+\\b', text)\n",
        "\n",
        "# 2c.\n",
        "capitalized_words = re.findall(r'\\b[A-Z][a-z]*\\b', text)\n",
        "\n",
        "# 3a.\n",
        "alpha_words = re.findall(r'\\b[a-zA-Z]+\\b', text)\n",
        "\n",
        "# 3b.\n",
        "vowel_words = [word for word in alpha_words if re.match(r'^[aeiouAEIOU]', word)]\n",
        "\n",
        "print(\"Words with more than 5 letters:\\n\", words_over_5)\n",
        "print(\"\\nNumbers in text:\\n\", numbers)\n",
        "print(\"\\nCapitalized Words:\\n\", capitalized_words)\n",
        "print(\"\\nAlphabetic Words Only:\\n\", alpha_words)\n",
        "print(\"\\nWords Starting with Vowels:\\n\", vowel_words)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fFs186xJxx_U",
        "outputId": "29ad7819-2f2c-46ae-dbf7-78d5dd01aedf"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Words with more than 5 letters:\n",
            " ['always', 'passion', 'exploring', 'different', 'cuisines', 'around', 'especially', 'Indian', 'curries', 'savory', 'Italian', 'pastas', 'experience', 'tasting', 'dishes', 'excites', 'whether', 'street', 'gourmet', 'Cooking', 'creativity', 'satisfaction', 'experimenting', 'flavors', 'textures', 'ingredients', 'connect', 'people']\n",
            "\n",
            "Numbers in text:\n",
            " []\n",
            "\n",
            "Capitalized Words:\n",
            " ['Food', 'I', 'Indian', 'Italian', 'The', 'Cooking', 'I', 'To']\n",
            "\n",
            "Alphabetic Words Only:\n",
            " ['Food', 'has', 'always', 'been', 'a', 'passion', 'of', 'mine', 'I', 'love', 'exploring', 'different', 'cuisines', 'from', 'around', 'the', 'world', 'especially', 'spicy', 'Indian', 'curries', 'and', 'savory', 'Italian', 'pastas', 'The', 'experience', 'of', 'tasting', 'new', 'dishes', 'excites', 'me', 'whether', 'it', 's', 'street', 'food', 'or', 'gourmet', 'meals', 'Cooking', 'at', 'home', 'also', 'gives', 'me', 'a', 'sense', 'of', 'creativity', 'and', 'satisfaction', 'I', 'enjoy', 'experimenting', 'with', 'flavors', 'textures', 'and', 'ingredients', 'To', 'me', 'food', 'is', 'not', 'just', 'fuel', 'but', 'an', 'art', 'form', 'and', 'a', 'way', 'to', 'connect', 'with', 'people']\n",
            "\n",
            "Words Starting with Vowels:\n",
            " ['always', 'a', 'of', 'I', 'exploring', 'around', 'especially', 'Indian', 'and', 'Italian', 'experience', 'of', 'excites', 'it', 'or', 'at', 'also', 'a', 'of', 'and', 'I', 'enjoy', 'experimenting', 'and', 'ingredients', 'is', 'an', 'art', 'and', 'a']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q4. Custom Tokenization & Regex-based Text Cleaning\n",
        "1. Take original text from Question 1.\n",
        "2. Write a custom tokenization function that:\n",
        "\n",
        "  a. Removes punctuation and special symbols, but keeps contractions (e.g., \"isn't\" should not be split into \"is\" and \"n't\").\n",
        "\n",
        "  b. Handles hyphenated words as a single token (e.g., \"state-of-the-art\" remains a single token).\n",
        "\n",
        "  c. Tokenizes numbers separately but keeps decimal numbers intact (e.g., \"3.14\" should remain as is).\n",
        "3. Use Regex Substitutions (re.sub) to:\n",
        "\n",
        "  a. Replace email addresses with '<EMAIL>' placeholder.\n",
        "\n",
        "  b. Replace URLs with '<URL>' placeholder.\n",
        "\n",
        "  c. Replace phone numbers (formats: 123-456-7890 or +91 9876543210) with\n",
        "'<PHONE>' placeholder."
      ],
      "metadata": {
        "id": "H4XdzSGhyULL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 1.\n",
        "text1 = \"\"\"\n",
        "Food has always been a passion of mine. I love exploring different cuisines from around the world,\n",
        "especially spicy Indian curries and savory Italian pastas. The experience of tasting new dishes excites me,\n",
        "whether it's street food or gourmet meals. Cooking at home also gives me a sense of creativity and satisfaction.\n",
        "I enjoy experimenting with flavors, textures, and ingredients. To me, food is not just fuel,\n",
        "but an art form and a way to connect with people.\n",
        "\n",
        "Contact me at foodlover@example.com or visit https://foodblogger.net for more recipes!\n",
        "You can also call +91 9876543210 or 123-456-7890 for collaborations.\n",
        "\"\"\"\n"
      ],
      "metadata": {
        "id": "4u6g-Bf_yvnl"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "\n",
        "# 2.\n",
        "def custom_tokenizer(text1):\n",
        "    # a. Keep contractions like \"isn't\"\n",
        "    # b. Handles hyphenated words like \"state-of-the-art\"\n",
        "    # c. Tokenize decimal numbers like 3.14\n",
        "    pattern = r\"\"\"\n",
        "        \\b\\d+\\.\\d+\\b              # decimal numbers\n",
        "        |\\b\\d+\\b                  # integers\n",
        "        |\\b\\w+(?:[-']\\w+)*\\b      # words with contractions or hyphens\n",
        "    \"\"\"\n",
        "    tokens = re.findall(pattern, text1, re.VERBOSE)\n",
        "    return tokens"
      ],
      "metadata": {
        "id": "xOQqfYMGy_S2"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 3.\n",
        "def clean_text(text1):\n",
        "    # a. Replace email addresses\n",
        "    text1 = re.sub(r'\\b[\\w.-]+?@\\w+?\\.\\w{2,4}\\b', '<EMAIL>', text1)\n",
        "\n",
        "    # b. Replace URLs\n",
        "    text1 = re.sub(r'https?://[^\\s]+', '<URL>', text1)\n",
        "\n",
        "    # c. Replace phone numbers\n",
        "    text1 = re.sub(r'\\+?\\d{1,3}[-\\s]?\\d{10}|\\d{3}[-]\\d{3}[-]\\d{4}', '<PHONE>', text1)\n",
        "\n",
        "    return text1\n"
      ],
      "metadata": {
        "id": "w15hOmv7zOZi"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cleaned_text = clean_text(text1)\n",
        "\n",
        "tokens = custom_tokenizer(cleaned_text)\n",
        "\n",
        "print(\"Cleaned Text:\\n\", cleaned_text)\n",
        "print(\"\\nCustom Tokens:\\n\", tokens)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XO10zLlIzooF",
        "outputId": "a73dddd0-12e2-4e93-ca5a-3d16129dbfd1"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cleaned Text:\n",
            " \n",
            "Food has always been a passion of mine. I love exploring different cuisines from around the world, \n",
            "especially spicy Indian curries and savory Italian pastas. The experience of tasting new dishes excites me, \n",
            "whether it's street food or gourmet meals. Cooking at home also gives me a sense of creativity and satisfaction. \n",
            "I enjoy experimenting with flavors, textures, and ingredients. To me, food is not just fuel, \n",
            "but an art form and a way to connect with people.\n",
            "\n",
            "Contact me at <EMAIL> or visit <URL> for more recipes! \n",
            "You can also call <PHONE> or <PHONE> for collaborations.\n",
            "\n",
            "\n",
            "Custom Tokens:\n",
            " ['Food', 'has', 'always', 'been', 'a', 'passion', 'of', 'mine', 'I', 'love', 'exploring', 'different', 'cuisines', 'from', 'around', 'the', 'world', 'especially', 'spicy', 'Indian', 'curries', 'and', 'savory', 'Italian', 'pastas', 'The', 'experience', 'of', 'tasting', 'new', 'dishes', 'excites', 'me', 'whether', \"it's\", 'street', 'food', 'or', 'gourmet', 'meals', 'Cooking', 'at', 'home', 'also', 'gives', 'me', 'a', 'sense', 'of', 'creativity', 'and', 'satisfaction', 'I', 'enjoy', 'experimenting', 'with', 'flavors', 'textures', 'and', 'ingredients', 'To', 'me', 'food', 'is', 'not', 'just', 'fuel', 'but', 'an', 'art', 'form', 'and', 'a', 'way', 'to', 'connect', 'with', 'people', 'Contact', 'me', 'at', 'EMAIL', 'or', 'visit', 'URL', 'for', 'more', 'recipes', 'You', 'can', 'also', 'call', 'PHONE', 'or', 'PHONE', 'for', 'collaborations']\n"
          ]
        }
      ]
    }
  ]
}